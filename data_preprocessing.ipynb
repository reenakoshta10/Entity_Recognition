{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reutersNLTK.xlsx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('Assets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the first text of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "df = pd.read_excel('Assets/reutersNLTK.xlsx')\n",
    "first_text = df.iloc[0,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    text_list.append(df.iloc[i,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = ' '.join(re.sub(' +', ' ',first_text.replace('\\'s','').replace('\\'t','').replace('&lt;','').replace(\">\",\"\")).split('\\n')[1:]).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spacy. load() is a convenience wrapper that reads the pipeline's config. cfg , uses the language and pipeline information to construct a Language object, loads in the model data and weights, and returns it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol\n"
     ]
    }
   ],
   "source": [
    "def text_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "print(text_preprocessor(\"LOL\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  accented_char_removal(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "normalized_text = accented_char_removal(normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mounting trade friction between the  U.S. And Japan has raised fears among many of Asia exporting  nations that the row could inflict far-reaching economic  damage, businessmen and officials said.  They told Reuter correspondents in Asian capitals a U.S.  Move against Japan might boost protectionist sentiment in the  U.S. And lead to curbs on American imports of their products.  But some exporters said that while the conflict would hurt  them in the long-run, in the short-term Tokyo loss might be  their gain.  The U.S. Has said it will impose 300 mln dlrs of tariffs on  imports of Japanese electronics goods on April 17, in  retaliation for Japan alleged failure to stick to a pact not  to sell semiconductors on world markets at below cost.  Unofficial Japanese estimates put the impact of the tariffs  at 10 billion dlrs and spokesmen for major electronics firms  said they would virtually halt exports of products hit by the  new taxes.  \"We wouldn be able to do business,\" said a spokesman for  leading Japanese electronics firm Matsushita Electric  Industrial Co Ltd MC.T.  \"If the tariffs remain in place for any length of time  beyond a few months it will mean the complete erosion of  exports (of goods subject to tariffs) to the U.S.,\" said Tom  Murtha, a stock analyst at the Tokyo office of broker James  Capel and Co.  In Taiwan, businessmen and officials are also worried.  \"We are aware of the seriousness of the U.S. Threat against  Japan because it serves as a warning to us,\" said a senior  Taiwanese trade official who asked not to be named.  Taiwan had a trade trade surplus of 15.6 billion dlrs last  year, 95 pct of it with the U.S.  The surplus helped swell Taiwan foreign exchange reserves  to 53 billion dlrs, among the world largest.  \"We must quickly open our markets, remove trade barriers and  cut import tariffs to allow imports of U.S. Products, if we  want to defuse problems from possible U.S. Retaliation,\" said  Paul Sheen, chairman of textile exporters Taiwan Safe Group.  A senior official of South Korea trade promotion  association said the trade dispute between the U.S. And Japan  might also lead to pressure on South Korea, whose chief exports  are similar to those of Japan.  Last year South Korea had a trade surplus of 7.1 billion  dlrs with the U.S., Up from 4.9 billion dlrs in 1985.  In Malaysia, trade officers and businessmen said tough  curbs against Japan might allow hard-hit producers of  semiconductors in third countries to expand their sales to the  U.S.  In Hong Kong, where newspapers have alleged Japan has been  selling below-cost semiconductors, some electronics  manufacturers share that view. But other businessmen said such  a short-term commercial advantage would be outweighed by  further U.S. Pressure to block imports.  \"That is a very short-term view,\" said Lawrence Mills,  director-general of the Federation of Hong Kong Industry.  \"If the whole purpose is to prevent imports, one day it will  be extended to other sources. Much more serious for Hong Kong  is the disadvantage of action restraining trade,\" he said.  The U.S. Last year was Hong Kong biggest export market,  accounting for over 30 pct of domestically produced exports.  The Australian government is awaiting the outcome of trade  talks between the U.S. And Japan with interest and concern,  Industry Minister John Button said in Canberra last Friday.  \"This kind of deterioration in trade relations between two  countries which are major trading partners of ours is a very  serious matter,\" Button said.  He said Australia concerns centred on coal and beef,  Australia two largest exports to Japan and also significant  U.S. Exports to that country.  Meanwhile U.S.-Japanese diplomatic manoeuvres to solve the  trade stand-off continue.  Japan ruling Liberal Democratic Party yesterday outlined  a package of economic measures to boost the Japanese economy.  The measures proposed include a large supplementary budget  and record public works spending in the first half of the  financial year.  They also call for stepped-up spending as an emergency  measure to stimulate the economy despite Prime Minister  Yasuhiro Nakasone avowed fiscal reform program.  Deputy U.S. Trade Representative Michael Smith and Makoto  Kuroda, Japan deputy minister of International Trade and  Industry (MITI), are due to meet in Washington this week in an  effort to end the dispute.'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "normalized_text = expand_contractions(normalized_text)\n",
    "normalized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_list = []\n",
    "    for token in doc:\n",
    "        lemmatized_list.append(token.lemma_)\n",
    "    \n",
    "    return lemmatized_list\n",
    "\n",
    "\n",
    "len(lemmatize_text(normalized_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apples', 'and', 'oranges', 'are', 'similar', '.', 'Boots', 'and', 'hippos', 'are', \"n't\", '.']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    tokenized_list = [token.text for token in doc]\n",
    "    return tokenized_list\n",
    "print(tokenize_text(u\"Apples and oranges are similar. Boots and hippos aren't.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stopwords = nlp.Defaults.stop_words\n",
    "\",\" in all_stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', ',', 'stopword', ',', 'computer']\n"
     ]
    }
   ],
   "source": [
    "def stopword_removal(text):\n",
    "    all_stopwords = nlp.Defaults.stop_words\n",
    "    \n",
    "    #all_stopwords.remove('not')\n",
    "\n",
    "    text_tokens = lemmatize_text(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords]\n",
    "\n",
    "    print(tokens_without_sw)\n",
    "\n",
    "stopword_removal(\"The, and, if are stopwords, computer is not\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_docs(documents):\n",
    "\n",
    "    normalized_documents = []\n",
    "    for doc in documents:\n",
    "        doc = expand_contractions(doc)\n",
    "        doc = ' '.join(re.sub(' +', ' ',doc.replace('\\'s','').replace('\\'t','').replace('&lt;','').replace(\">\",\"\")).split('\\n')[1:]).strip()\n",
    "        normalized_documents.append(doc)\n",
    "    return normalized_documents\n",
    "\n",
    "    \n",
    "list_normalized_text = normalize_docs(text_list)\n",
    "\n",
    "df['text'] = list_normalized_text\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('Assets/preprocessed_text.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ids</th>\n",
       "      <th>categories</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test/14826</td>\n",
       "      <td>['trade']</td>\n",
       "      <td>Mounting trade friction between the  U.S. And ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test/14828</td>\n",
       "      <td>['grain']</td>\n",
       "      <td>A survey of 19 provinces and seven cities  sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test/14829</td>\n",
       "      <td>['crude', 'nat-gas']</td>\n",
       "      <td>The Ministry of International Trade and  Indus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test/14832</td>\n",
       "      <td>['corn', 'grain', 'rice', 'rubber', 'sugar', '...</td>\n",
       "      <td>Thailands trade deficit widened to 4.5  billio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test/14833</td>\n",
       "      <td>['palm-oil', 'veg-oil']</td>\n",
       "      <td>Indonesia expects crude palm oil (CPO)  prices...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10783</th>\n",
       "      <td>training/999</td>\n",
       "      <td>['interest', 'money-fx']</td>\n",
       "      <td>The Bank of England said it had revised  its f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10784</th>\n",
       "      <td>training/9992</td>\n",
       "      <td>['earn']</td>\n",
       "      <td>Qtly div 25 cts vs 25 cts prior  Pay April 13 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>training/9993</td>\n",
       "      <td>['earn']</td>\n",
       "      <td>Qtly div 12 cts vs 12 cts prior  Pay April 21 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>training/9994</td>\n",
       "      <td>['earn']</td>\n",
       "      <td>Shr loss six cts vs loss 18 cts  Net loss 89,4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>training/9995</td>\n",
       "      <td>['earn']</td>\n",
       "      <td>Shr 43 cts vs 52 cts  Shr diluted 41 cts vs 49...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10788 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ids                                         categories  \\\n",
       "0         test/14826                                          ['trade']   \n",
       "1         test/14828                                          ['grain']   \n",
       "2         test/14829                               ['crude', 'nat-gas']   \n",
       "3         test/14832  ['corn', 'grain', 'rice', 'rubber', 'sugar', '...   \n",
       "4         test/14833                            ['palm-oil', 'veg-oil']   \n",
       "...              ...                                                ...   \n",
       "10783   training/999                           ['interest', 'money-fx']   \n",
       "10784  training/9992                                           ['earn']   \n",
       "10785  training/9993                                           ['earn']   \n",
       "10786  training/9994                                           ['earn']   \n",
       "10787  training/9995                                           ['earn']   \n",
       "\n",
       "                                                    text  \n",
       "0      Mounting trade friction between the  U.S. And ...  \n",
       "1      A survey of 19 provinces and seven cities  sho...  \n",
       "2      The Ministry of International Trade and  Indus...  \n",
       "3      Thailands trade deficit widened to 4.5  billio...  \n",
       "4      Indonesia expects crude palm oil (CPO)  prices...  \n",
       "...                                                  ...  \n",
       "10783  The Bank of England said it had revised  its f...  \n",
       "10784  Qtly div 25 cts vs 25 cts prior  Pay April 13 ...  \n",
       "10785  Qtly div 12 cts vs 12 cts prior  Pay April 21 ...  \n",
       "10786  Shr loss six cts vs loss 18 cts  Net loss 89,4...  \n",
       "10787  Shr 43 cts vs 52 cts  Shr diluted 41 cts vs 49...  \n",
       "\n",
       "[10788 rows x 3 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_excel('Assets/preprocessed_text.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def remove_special_characters(text, remove_digits=False):\n",
    "#     pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "#     text = re.sub(pattern, '', text)\n",
    "#     return text\n",
    "\n",
    "# remove_special_characters(\"Well this was fun! What do you think?\", \n",
    "#                           remove_digits=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "993ba414554bd467ec8a9ec2ba0eb2bb6024f51dff34b0a39558c3dc8d521aef"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('Entity_Recognition': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
